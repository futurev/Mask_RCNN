
J.Xu

20190925

-1 config <train> or <infer>
-2 build network
  --  # RPN GT
      input_rpn_match
      input_rpn_bbox
   
   -- Detection GT (class IDs, bounding boxes, and masks)
      
      ---input
            # 1. GT Class IDs (zero padded)
            input_gt_class_ids
             # 2. GT Boxes in pixels (zero padded)
            # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates
            input_gt_boxes
            # Normalize coordinates
            gt_boxes
            # 3. GT Masks (zero padded)
            # [batch, height, width, MAX_GT_INSTANCES]
            config.USE_MINI_MASK:
                input_gt_masks
      
      ---backbone
            rpn_feature_maps = [P2, P3, P4, P5, P6]
            mrcnn_feature_maps = [P2, P3, P4, P5]
      
      ---anchors
          anchors <class 'tuple'>: (2, 261888, 4) 261888 = 512^2 + 256^2 + ...+32^2
          
      ---rpn network, output: rpn_rois
          ----rpn_graph(feature_map, anchors_per_location, anchor_stride)
              feature_map: backbone features [batch, height, width, depth]
              anchors_per_location: number of anchors per pixel in the feature map
              anchor_stride: Controls the density of anchors. Typically 1 (anchors for
                             every pixel in the feature map), or 2 (every other pixel).

              Returns:
                  rpn_class_logits: [batch, H * W * anchors_per_location, 2] Anchor classifier logits (before softmax)
                  rpn_probs: [batch, H * W * anchors_per_location, 2] Anchor classifier probabilities.
                  rpn_bbox: [batch, H * W * anchors_per_location, (dy, dx, log(dh), log(dw))] Deltas to be
                            applied to anchors.
                            
           ----get output from rpn
                # Loop through pyramid layers [p] and append to layer_outputs 
                # Concatenate layer outputs
                # Convert from list of lists of level outputs to list of lists
                # of outputs across levels.
                # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]
                get final result from rpn:
                rpn_class_logits, rpn_class, rpn_bbox
                
           ----get proposals
                # Generate proposals
                # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates
                # and zero padded. proposal_count = 2000
                
                PropsalLayer()
                """Receives anchor scores and selects a subset to pass as proposals
                  to the second stage. Filtering is done based on anchor scores and
                  non-max suppression to remove overlaps. It also applies bounding
                  box refinement deltas to anchors.

                  Inputs:
                      rpn_probs: [batch, num_anchors, (bg prob, fg prob)]
                      rpn_bbox: [batch, num_anchors, (dy, dx, log(dh), log(dw))]
                      anchors: [batch, num_anchors, (y1, x1, y2, x2)] anchors in normalized coordinates

                  Returns:
                      Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)]  ==> rpn_rois
                      
                  -----scores: Improve performance by trimming to top anchors by score
                                # and doing the rest on the smaller subset.
                  -----delta: # Apply deltas to anchors to get refined anchors.
                        # [batch, N, (y1, x1, y2, x2)]
                  -----clip: # Clip to image boundaries. Since we're in normalized coordinates,
                              # clip to 0..1 range. [batch, N, (y1, x1, y2, x2)]
                  -----nms: # Non-max suppression
                if config.use_rpn_rois: rpn_rois==>target_rois  
                otherwise, use all 2000 roi
  
        ---detection network  ==> rois, target_class_ids, target_bbox, target_mask 
            
            rois, target_class_ids, target_bbox, target_mask =\
                DetectionTargetLayer(config, name="proposal_targets")([
                    target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])
                    
            Subsamples proposals and generates target box refinement, class_ids,
            and masks for each.

            Inputs:
            proposals: [batch, N, (y1, x1, y2, x2)] in normalized coordinates. Might
                       be zero padded if there are not enough proposals.
            gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs.
            gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized
                      coordinates.
            gt_masks: [batch, height, width, MAX_GT_INSTANCES] of boolean type

            Returns: Target ROIs and corresponding class IDs, bounding box shifts,
            and masks.
            rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized
                  coordinates
            target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.
            target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw)]
            target_mask: [batch, TRAIN_ROIS_PER_IMAGE, height, width]
                         Masks cropped to bbox boundaries and resized to neural
                          network output size.
            Note: Returned arrays might be zero padded if not enough target ROIs.              
                rois, target_class_ids, target_bbox, target_mask =\
                DetectionTargetLayer(config, name="proposal_targets")([
                    target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])
                  
        
        ---mask network
           ----fpn_classifier_graph  ==> mrcnn_class_logits, mrcnn_class, mrcnn_bbox
                
                 mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\
                fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta,
                                     config.POOL_SIZE, config.NUM_CLASSES,
                                     train_bn=config.TRAIN_BN,
                                     fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)
                
                Builds the computation graph of the feature pyramid network classifier
                and regressor heads.

                rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized
                      coordinates.
                feature_maps: List of feature maps from different layers of the pyramid,
                              [P2, P3, P4, P5]. Each has a different resolution.
                image_meta: [batch, (meta data)] Image details. See compose_image_meta()
                pool_size: The width of the square feature map generated from ROI Pooling.
                num_classes: number of classes, which determines the depth of the results
                train_bn: Boolean. Train or freeze Batch Norm layers
                fc_layers_size: Size of the 2 FC layers

                Returns:
                    logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)
                    probs: [batch, num_rois, NUM_CLASSES] classifier probabilities
                    bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to
                                 proposal boxes
           ----mask  ==> mrcnn_mask
                Builds the computation graph of the mask head of Feature Pyramid Network.

                mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps,
                                              input_image_meta,
                                              config.MASK_POOL_SIZE,
                                              config.NUM_CLASSES,
                                              train_bn=config.TRAIN_BN)

                rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized
                      coordinates.
                feature_maps: List of feature maps from different layers of the pyramid,
                              [P2, P3, P4, P5]. Each has a different resolution.
                image_meta: [batch, (meta data)] Image details. See compose_image_meta()
                pool_size: The width of the square feature map generated from ROI Pooling.
                num_classes: number of classes, which determines the depth of the results
                train_bn: Boolean. Train or freeze Batch Norm layers

                Returns: Masks [batch, num_rois, MASK_POOL_SIZE, MASK_POOL_SIZE, NUM_CLASSES]
                    ---Losses
                        rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name="rpn_class_loss")(
                            [input_rpn_match, rpn_class_logits])
                        rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name="rpn_bbox_loss")(
                            [input_rpn_bbox, input_rpn_match, rpn_bbox])
                        class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name="mrcnn_class_loss")(
                            [target_class_ids, mrcnn_class_logits, active_class_ids])
                        bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name="mrcnn_bbox_loss")(
                            [target_bbox, target_class_ids, mrcnn_bbox])
                        mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name="mrcnn_mask_loss")(
                            [target_mask, target_class_ids, mrcnn_mask])
               1) ROI Pooling
                # Shape: [batch, num_rois, MASK_POOL_SIZE, MASK_POOL_SIZE, channels]
                x = PyramidROIAlign()
               2) 4 conv + relu and 2 conv + return 
                
        ---loss
        # Losses
            rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name="rpn_class_loss")(
                [input_rpn_match, rpn_class_logits])
                
                
            rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name="rpn_bbox_loss")(
                [input_rpn_bbox, input_rpn_match, rpn_bbox])
                
                
            class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name="mrcnn_class_loss")(
                [target_class_ids, mrcnn_class_logits, active_class_ids])
                
                
            bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name="mrcnn_bbox_loss")(
                [target_bbox, target_class_ids, mrcnn_bbox])
                
                
            mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name="mrcnn_mask_loss")(
                [target_mask, target_class_ids, mrcnn_mask])
        
-3 load_weight
-4 train


0930

data_prepare

mask and bbox
model.train() --> data-generator() --> load_image_gt()
